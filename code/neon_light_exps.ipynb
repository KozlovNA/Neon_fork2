{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "neon_light.py\n",
    "A lightweight version of the Muon optimizer using a perceptron model\n",
    "Runs on AMD Radeon GPUs using ROCm\n",
    "\"\"\"\n",
    "from os import putenv\n",
    "putenv(\"HSA_OVERRIDE_GFX_VERSION\", \"9.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from math import ceil\n",
    "\n",
    "# Enable ROCm backend\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#               Muon optimizer              #\n",
    "#############################################\n",
    "\n",
    "def zeropower_via_newtonschulz5(G, steps=3, eps=1e-7):\n",
    "    \"\"\"Simplified Newton-Schulz iteration for whitening\"\"\"\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    X = G.bfloat16()\n",
    "    X /= (X.norm() + eps)\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        B = b * A + c * A @ A\n",
    "        X = a * X + B @ X\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "class Muon(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, momentum=0, nesterov=False):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "        if nesterov and momentum <= 0:\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum\")\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            momentum = group['momentum']\n",
    "            for p in group['params']:\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "\n",
    "                if 'momentum_buffer' not in state.keys():\n",
    "                    state['momentum_buffer'] = torch.zeros_like(g)\n",
    "                buf = state['momentum_buffer']\n",
    "                buf.mul_(momentum).add_(g)\n",
    "                g = g.add(buf, alpha=momentum) if group['nesterov'] else buf\n",
    "\n",
    "                p.data.mul_(len(p.data)**0.5 / p.data.norm())\n",
    "                update = zeropower_via_newtonschulz5(g.reshape(len(g), -1)).view(g.shape)\n",
    "                p.data.add_(update, alpha=-lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def u1s1v1t(W, num_iter=30):\n",
    "    v = np.random.randn(W.shape[1])\n",
    "    v /= np.linalg.norm(v)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        u = W @ v\n",
    "        u /= np.linalg.norm(u)\n",
    "        v = W.T @ u\n",
    "        v /= np.linalg.norm(v)\n",
    "    \n",
    "    return u.T @ W @ v\n",
    "\n",
    "class Neon(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, momentum=0, nesterov=False):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "        if nesterov and momentum <= 0:\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum\")\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            momentum = group['momentum']\n",
    "            for p in group['params']:\n",
    "                g = p.grad\n",
    "                if g is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "\n",
    "                if 'momentum_buffer' not in state.keys():\n",
    "                    state['momentum_buffer'] = torch.zeros_like(g)\n",
    "                buf = state['momentum_buffer']\n",
    "                buf.mul_(momentum).add_(g)\n",
    "                g = g.add(buf, alpha=momentum) if group['nesterov'] else buf\n",
    "\n",
    "                p.data.mul_(len(p.data)**0.5 / p.data.norm())\n",
    "                update = u1s1v1t(g.reshape(len(g), -1)).view(g.shape)\n",
    "                p.data.add_(update, alpha=-lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#                DataLoader                 #\n",
    "#############################################\n",
    "\n",
    "CIFAR_MEAN = torch.tensor((0.4914, 0.4822, 0.4465))\n",
    "CIFAR_STD = torch.tensor((0.2470, 0.2435, 0.2616))\n",
    "\n",
    "def batch_flip_lr(inputs):\n",
    "    flip_mask = (torch.rand(len(inputs), device=inputs.device) < 0.5).view(-1, 1, 1, 1)\n",
    "    return torch.where(flip_mask, inputs.flip(-1), inputs)\n",
    "\n",
    "def batch_crop(images, crop_size):\n",
    "    r = (images.size(-1) - crop_size)//2\n",
    "    shifts = torch.randint(-r, r+1, size=(len(images), 2), device=images.device)\n",
    "    images_out = torch.empty((len(images), 3, crop_size, crop_size), device=images.device, dtype=images.dtype)\n",
    "    if r <= 2:\n",
    "        for sy in range(-r, r+1):\n",
    "            for sx in range(-r, r+1):\n",
    "                mask = (shifts[:, 0] == sy) & (shifts[:, 1] == sx)\n",
    "                images_out[mask] = images[mask, :, r+sy:r+sy+crop_size, r+sx:r+sx+crop_size]\n",
    "    else:\n",
    "        images_tmp = torch.empty((len(images), 3, crop_size, crop_size+2*r), device=images.device, dtype=images.dtype)\n",
    "        for s in range(-r, r+1):\n",
    "            mask = (shifts[:, 0] == s)\n",
    "            images_tmp[mask] = images[mask, :, r+s:r+s+crop_size, :]\n",
    "        for s in range(-r, r+1):\n",
    "            mask = (shifts[:, 1] == s)\n",
    "            images_out[mask] = images_tmp[mask, :, :, r+s:r+s+crop_size]\n",
    "    return images_out\n",
    "\n",
    "class CifarLoader:\n",
    "    def __init__(self, path, train=True, batch_size=500, aug=None):\n",
    "        data_path = os.path.join(path, 'train.pt' if train else 'test.pt')\n",
    "        if not os.path.exists(data_path):\n",
    "            dset = torchvision.datasets.CIFAR10(path, download=True, train=train)\n",
    "            images = torch.tensor(dset.data)\n",
    "            labels = torch.tensor(dset.targets)\n",
    "            torch.save({'images': images, 'labels': labels, 'classes': dset.classes}, data_path)\n",
    "\n",
    "        data = torch.load(data_path, map_location='cpu')  # Load to CPU first\n",
    "        self.images, self.labels, self.classes = data['images'], data['labels'], data['classes']\n",
    "        # Convert to float32 instead of half\n",
    "        self.images = (self.images.float() / 255).permute(0, 3, 1, 2).to(memory_format=torch.channels_last)\n",
    "\n",
    "        self.normalize = T.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "        self.proc_images = {}\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.aug = aug or {}\n",
    "        for k in self.aug.keys():\n",
    "            assert k in ['flip', 'translate'], 'Unrecognized key: %s' % k\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = train\n",
    "        self.shuffle = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)//self.batch_size if self.drop_last else ceil(len(self.images)/self.batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.epoch == 0:\n",
    "            images = self.proc_images['norm'] = self.normalize(self.images)\n",
    "            if self.aug.get('flip', False):\n",
    "                images = self.proc_images['flip'] = batch_flip_lr(images)\n",
    "            pad = self.aug.get('translate', 0)\n",
    "            if pad > 0:\n",
    "                self.proc_images['pad'] = F.pad(images, (pad,)*4, 'reflect')\n",
    "\n",
    "        if self.aug.get('translate', 0) > 0:\n",
    "            images = batch_crop(self.proc_images['pad'], self.images.shape[-2])\n",
    "        elif self.aug.get('flip', False):\n",
    "            images = self.proc_images['flip']\n",
    "        else:\n",
    "            images = self.proc_images['norm']\n",
    "        if self.aug.get('flip', False):\n",
    "            if self.epoch % 2 == 1:\n",
    "                images = images.flip(-1)\n",
    "\n",
    "        self.epoch += 1\n",
    "\n",
    "        indices = (torch.randperm if self.shuffle else torch.arange)(len(images), device='cpu')\n",
    "        for i in range(len(self)):\n",
    "            idxs = indices[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            yield (images[idxs].to(device), self.labels[idxs].to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#            Network Definition             #\n",
    "#############################################\n",
    "\n",
    "class SimplePerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(32*32*3, 512)\n",
    "        self.linear2 = nn.Linear(512, 10)\n",
    "        self.activ = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.activ(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#                Training                  #\n",
    "############################################\n",
    "\n",
    "def train_model(model, optimizers, train_loader, test_loader, total_epochs):\n",
    "    start_time = time.time()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(total_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            for opt in optimizers:\n",
    "                opt.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            for opt in optimizers:\n",
    "                opt.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        train_acc = 100. * correct / total\n",
    "        print(f\"Train Loss: {total_loss/len(train_loader):.3f} | Train Acc: {train_acc:.3f}%\")\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        test_acc = 100. * correct / total\n",
    "        print(f\"Test Acc: {test_acc:.3f}%\")\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    return best_acc, training_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    batch_size = 128\n",
    "    total_epochs = 5\n",
    "    wd = 2e-6 * batch_size  # weight decay\n",
    "    bias_lr = 0.053\n",
    "    head_lr = 0.1\n",
    "\n",
    "    train_loader = CifarLoader('cifar10', train=True, batch_size=batch_size, aug=dict(flip=True, translate=2))\n",
    "    test_loader = CifarLoader('cifar10', train=False, batch_size=batch_size)\n",
    "\n",
    "    # Train with Muon optimizer\n",
    "    print(\"\\nTraining with Muon optimizer...\")\n",
    "    model_muon = SimplePerceptron().to(device)\n",
    "    \n",
    "    # Configure optimizers for perceptron model\n",
    "    # Split parameters into linear layers and biases\n",
    "    linear_params = [p for n, p in model_muon.named_parameters() if 'weight' in n]\n",
    "    bias_params = [p for n, p in model_muon.named_parameters() if 'bias' in n]\n",
    "    \n",
    "    param_configs = [\n",
    "        dict(params=[model_muon.linear2.weight], lr=head_lr, weight_decay=wd/head_lr),\n",
    "        dict(params=bias_params, lr=bias_lr, weight_decay=wd/bias_lr)\n",
    "    ]\n",
    "    optimizer1 = torch.optim.SGD(param_configs, momentum=0.85, nesterov=True)\n",
    "    optimizer2 = Muon(linear_params, lr=0.24, momentum=0.6, nesterov=True)\n",
    "    optimizer3 = Muon(linear_params, lr=0.24, momentum=0.6, nesterov=True)\n",
    "    optimizers_muon = [optimizer3, optimizer1, optimizer2]\n",
    "    \n",
    "    for opt in optimizers_muon:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"initial_lr\"] = group[\"lr\"]\n",
    "    \n",
    "    muon_acc, muon_time = train_model(model_muon, optimizers_muon, train_loader, test_loader, total_epochs)\n",
    "    print(f\"\\nMuon Results:\")\n",
    "    print(f\"Best Accuracy: {muon_acc:.2f}%\")\n",
    "    print(f\"Training Time: {muon_time:.2f} seconds\")\n",
    "\n",
    "    # Train with SGD optimizer\n",
    "    print(\"\\nTraining with SGD optimizer...\")\n",
    "    model_sgd = SimplePerceptron().to(device)\n",
    "    \n",
    "    # Configure SGD optimizer with the same parameter groups\n",
    "    param_configs_sgd = [\n",
    "        dict(params=[model_sgd.linear2.weight], lr=head_lr, weight_decay=wd/head_lr),\n",
    "        dict(params=bias_params, lr=bias_lr, weight_decay=wd/bias_lr),\n",
    "        dict(params=linear_params, lr=0.24, weight_decay=wd/0.24)\n",
    "    ]\n",
    "    optimizer_sgd = torch.optim.SGD(param_configs_sgd, momentum=0.85, nesterov=True)\n",
    "    \n",
    "    for group in optimizer_sgd.param_groups:\n",
    "        group[\"initial_lr\"] = group[\"lr\"]\n",
    "    \n",
    "    sgd_acc, sgd_time = train_model(model_sgd, [optimizer_sgd], train_loader, test_loader, total_epochs)\n",
    "    print(f\"\\nSGD Results:\")\n",
    "    print(f\"Best Accuracy: {sgd_acc:.2f}%\")\n",
    "    print(f\"Training Time: {sgd_time:.2f} seconds\")\n",
    "\n",
    "    # Print comparison\n",
    "    print(\"\\nComparison:\")\n",
    "    print(f\"Accuracy Difference: {muon_acc - sgd_acc:.2f}% (Muon - SGD)\")\n",
    "    print(f\"Time Difference: {muon_time - sgd_time:.2f} seconds (Muon - SGD)\")\n",
    "    print(f\"Speedup: {sgd_time/muon_time:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with Muon optimizer...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.13/site-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:310.)\n",
      "  return F.linear(input, self.weight, self.bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.585 | Train Acc: 20.230%\n",
      "Test Acc: 23.210%\n",
      "Epoch 2/5\n",
      "Train Loss: 5.807 | Train Acc: 20.685%\n",
      "Test Acc: 18.750%\n",
      "Epoch 3/5\n",
      "Train Loss: 5.575 | Train Acc: 20.773%\n",
      "Test Acc: 18.770%\n",
      "Epoch 4/5\n",
      "Train Loss: 5.437 | Train Acc: 20.998%\n",
      "Test Acc: 23.080%\n",
      "Epoch 5/5\n",
      "Train Loss: 5.466 | Train Acc: 21.084%\n",
      "Test Acc: 19.090%\n",
      "\n",
      "Muon Results:\n",
      "Best Accuracy: 23.21%\n",
      "Training Time: 102.31 seconds\n",
      "\n",
      "Training with SGD optimizer...\n",
      "Epoch 1/5\n",
      "Train Loss: 1.804 | Train Acc: 36.837%\n",
      "Test Acc: 41.230%\n",
      "Epoch 2/5\n",
      "Train Loss: 1.741 | Train Acc: 38.944%\n",
      "Test Acc: 41.330%\n",
      "Epoch 3/5\n",
      "Train Loss: 1.736 | Train Acc: 39.113%\n",
      "Test Acc: 41.140%\n",
      "Epoch 4/5\n",
      "Train Loss: 1.730 | Train Acc: 39.315%\n",
      "Test Acc: 41.250%\n",
      "Epoch 5/5\n",
      "Train Loss: 1.731 | Train Acc: 39.339%\n",
      "Test Acc: 40.950%\n",
      "\n",
      "SGD Results:\n",
      "Best Accuracy: 41.33%\n",
      "Training Time: 9.19 seconds\n",
      "\n",
      "Comparison:\n",
      "Accuracy Difference: -18.12% (Muon - SGD)\n",
      "Time Difference: 93.12 seconds (Muon - SGD)\n",
      "Speedup: 0.09x\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
